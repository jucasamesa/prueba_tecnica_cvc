# General Model Evaluator

## ğŸ¯ Overview

El `evaluate_model.py` es un evaluador general que puede trabajar con **cualquier modelo entrenado** para evaluar su rendimiento en los datos de validaciÃ³n.

## ğŸš€ CaracterÃ­sticas

- **Universal**: Funciona con cualquier modelo entrenado (Logistic Regression, SVC, Random Forest, CNN, etc.)
- **Logging automÃ¡tico**: Guarda todos los logs de evaluaciÃ³n en archivos timestamped
- **MÃ©tricas completas**: Accuracy, F1-Score, Precision, Recall, Confusion Matrix
- **AnÃ¡lisis por clase**: MÃ©tricas especÃ­ficas para cada clase
- **FÃ¡cil de usar**: Interfaz de lÃ­nea de comandos simple

## ğŸ“ Estructura de Archivos

```
models/
â”œâ”€â”€ background_logistic_regression_classifier_cv.pkl
â”œâ”€â”€ background_svc_classifier_cv.pkl
â”œâ”€â”€ background_random_forest_classifier_cv.pkl
â””â”€â”€ ...

logs/
â”œâ”€â”€ model_evaluation_20241201_143022.log
â”œâ”€â”€ model_evaluation_20241201_150145.log
â””â”€â”€ ...
```

## ğŸ® Uso

### 1. Listar modelos disponibles

```bash
python evaluate_model.py --list
# o
python evaluate_model.py -l
```

**Salida:**
```
ğŸ“ Available trained models:
   1. background_logistic_regression_classifier_cv.pkl
   2. background_svc_classifier_cv.pkl
   3. background_random_forest_classifier_cv.pkl
```

### 2. Evaluar un modelo especÃ­fico

```bash
# Especificar el modelo
python evaluate_model.py --model models/background_logistic_regression_classifier_cv.pkl

# O usar la ruta corta
python evaluate_model.py -m models/background_logistic_regression_classifier_cv.pkl
```

### 3. Evaluar el primer modelo disponible (automÃ¡tico)

```bash
python evaluate_model.py
```

### 4. Evaluar sin guardar logs

```bash
python evaluate_model.py --model models/background_logistic_regression_classifier_cv.pkl --no-log
```

## ğŸ“Š MÃ©tricas de EvaluaciÃ³n

El evaluador calcula y muestra:

### ğŸ¯ MÃ©tricas Generales
- **Accuracy**: PrecisiÃ³n general del modelo
- **F1-Score**: Media armÃ³nica de precisiÃ³n y recall
- **Precision**: ProporciÃ³n de predicciones positivas correctas
- **Recall**: ProporciÃ³n de casos positivos reales identificados

### ğŸ“ˆ MÃ©tricas por Clase
- **Background No Cumple (0)**: MÃ©tricas especÃ­ficas para fondos que no cumplen
- **Background Cumple (1)**: MÃ©tricas especÃ­ficas para fondos que cumplen

### ğŸ“Š Matriz de ConfusiÃ³n
```
   Predicted:
             0    1
   Actual 0: 245   12
         1:  23  156
```

### ğŸ“‹ Reporte Detallado
- ClasificaciÃ³n detallada por clase
- MÃ©tricas especÃ­ficas por clase
- AnÃ¡lisis de confianza del modelo

## ğŸ” Ejemplos de Uso

### Ejemplo 1: Evaluar modelo de RegresiÃ³n LogÃ­stica

```bash
python evaluate_model.py -m models/background_logistic_regression_classifier_cv.pkl
```

**Salida esperada:**
```
ğŸ¯ General Model Evaluator for Validation Data
==============================================================
ğŸ“ Log file: logs/model_evaluation_20241201_143022.log
ğŸ•’ Started at: 2024-12-01 14:30:22
==============================================================

ğŸ” Loading validation data...
âœ… Loaded 1380 validation mask arrays
âœ… Loaded validation mapping with 1380 entries
âœ… Loaded validation labels with 1380 entries
ğŸ”„ Processing validation mask arrays...
âœ… Processed 1380 validation samples
ğŸ“Š Validation data shape: (1380, 262144)
ğŸ“Š Validation labels shape: (1380,)

ğŸ” Loading trained model from models/background_logistic_regression_classifier_cv.pkl...
âœ… Model loaded successfully
ğŸ“‹ Model type: Pipeline(LogisticRegression)
ğŸš€ Making predictions on validation data...
âœ… Predictions completed successfully

==============================================================
ğŸ“Š MODEL EVALUATION ON VALIDATION DATA
==============================================================

ğŸ¤– MODEL INFORMATION:
   Model type: Pipeline(LogisticRegression)
   Model path: models/background_logistic_regression_classifier_cv.pkl

ğŸ¯ OVERALL PERFORMANCE:
   Accuracy:  0.9232 (92.32%)
   F1 Score:  0.9231
   Precision: 0.9234
   Recall:    0.9232

ğŸ“ˆ PER-CLASS PERFORMANCE:
   Background No Cumple (0):
     - F1 Score:  0.9234
     - Precision: 0.9232
     - Recall:    0.9236
   Background Cumple (1):
     - F1 Score:  0.9228
     - Precision: 0.9236
     - Recall:    0.9228

ğŸ“Š CONFUSION MATRIX:
   Predicted:
             0    1
   Actual 0: 245   12
         1:  23  156

ğŸ“‹ DETAILED CLASSIFICATION REPORT:
              precision    recall  f1-score   support

Background No Cumple       0.92      0.92      0.92       257
   Background Cumple       0.92      0.92      0.92       179

    accuracy                           0.92       436
   macro avg       0.92      0.92      0.92       436
weighted avg       0.92      0.92      0.92       436

ğŸ’¡ ADDITIONAL INSIGHTS:
   Total validation samples: 436
   Background No Cumple (0): 257 (58.9%)
   Background Cumple (1): 179 (41.1%)
   Average prediction confidence: 0.9234

==============================================================

âœ… Evaluation completed successfully!
ğŸ“ Model evaluated: models/background_logistic_regression_classifier_cv.pkl
ğŸ“Š Validation samples: 436
ğŸ“ Log saved to: logs/model_evaluation_20241201_143022.log

ğŸ•’ Completed at: 2024-12-01 14:30:25
```

### Ejemplo 2: Evaluar modelo SVC

```bash
python evaluate_model.py -m models/background_svc_classifier_cv.pkl
```

### Ejemplo 3: Evaluar modelo Random Forest

```bash
python evaluate_model.py -m models/background_random_forest_classifier_cv.pkl
```

## ğŸ“ Logs

Todos los logs se guardan automÃ¡ticamente en el directorio `logs/` con el formato:
```
logs/model_evaluation_YYYYMMDD_HHMMSS.log
```

### Contenido del Log
- InformaciÃ³n completa de la evaluaciÃ³n
- MÃ©tricas detalladas
- Matriz de confusiÃ³n
- Reporte de clasificaciÃ³n
- Errores y advertencias
- Timestamps de inicio y fin

## ğŸ”§ Opciones Avanzadas

### Argumentos Disponibles

| Argumento | DescripciÃ³n | Ejemplo |
|-----------|-------------|---------|
| `--model`, `-m` | Ruta al modelo a evaluar | `-m models/my_model.pkl` |
| `--list`, `-l` | Listar modelos disponibles | `-l` |
| `--no-log` | No guardar logs en archivo | `--no-log` |

### Combinaciones Ãštiles

```bash
# Listar modelos y luego evaluar uno especÃ­fico
python evaluate_model.py -l
python evaluate_model.py -m models/background_logistic_regression_classifier_cv.pkl

# Evaluar sin logs (solo terminal)
python evaluate_model.py -m models/background_svc_classifier_cv.pkl --no-log

# Evaluar el primer modelo disponible
python evaluate_model.py
```

## ğŸ¯ Ventajas del Evaluador General

1. **Reutilizable**: Un solo script para todos los modelos
2. **Consistente**: Mismas mÃ©tricas y formato para todos los modelos
3. **Mantenible**: Un solo lugar para actualizaciones
4. **Flexible**: FÃ¡cil de extender para nuevos tipos de modelos
5. **Documentado**: Logs completos para anÃ¡lisis posterior

## ğŸš¨ SoluciÃ³n de Problemas

### Error: "No trained models found"
```bash
# Verificar que existen modelos en el directorio models/
ls models/
```

### Error: "Model not found"
```bash
# Verificar la ruta exacta del modelo
python evaluate_model.py -l
```

### Error: "Validation data not found"
```bash
# Verificar que existen los datos de validaciÃ³n procesados
ls data/val_processed/
```

## ğŸ“ˆ ComparaciÃ³n de Modelos

Para comparar mÃºltiples modelos, ejecuta el evaluador para cada uno y revisa los logs:

```bash
# Evaluar todos los modelos disponibles
for model in models/*.pkl; do
    echo "Evaluating $model..."
    python evaluate_model.py -m "$model"
done
```

Luego compara los resultados en los logs generados.
